{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbImUjmR002XHB4mZTJZUn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clerfayt28/Pitonchik/blob/main/Exam3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\"\"\" import struct\n",
        "import sys\n",
        "from array import array\n",
        "from os import path\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./cache', train=True, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./cache', train=False, download=True)\n",
        "\n",
        "def read(dataset):\n",
        "    if dataset == \"training\":\n",
        "        path_img = \"./cache/MNIST/raw/train-images-idx3-ubyte\"\n",
        "        path_lbl = \"./cache/MNIST/raw/train-labels-idx1-ubyte\"\n",
        "    elif dataset == \"testing\":\n",
        "        path_img = \"./cache/MNIST/raw/t10k-images-idx3-ubyte\"\n",
        "        path_lbl = \"./cache/MNIST/raw/t10k-labels-idx1-ubyte\"\n",
        "    else:\n",
        "        raise ValueError(\"dataset must be 'testing' or 'training'\")\n",
        "\n",
        "    with open(path_lbl, 'rb') as f_lable:\n",
        "        __, size = struct.unpack(\">II\", f_lable.read(8))\n",
        "        lbl = array(\"B\", f_lable.read())\n",
        "\n",
        "    with open(path_img, 'rb') as f_img:\n",
        "        __, size, nrows, ncols = struct.unpack(\">IIII\", f_img.read(16))\n",
        "        img = array(\"B\", f_img.read())\n",
        "\n",
        "    return lbl, img, size, nrows, ncols\n",
        "\n",
        "def write_dataset(labels, data, size, rows, cols, output_dir):\n",
        "    classes = {i: f\"class{i}\" for i in range(10)}\n",
        "\n",
        "    output_dirs = [path.join(output_dir, classes[i]) for i in range(10)]\n",
        "\n",
        "    for dir in output_dirs:\n",
        "        dir = dir.replace(\"\\\\\", \"/\")\n",
        "        if not path.exists(dir):\n",
        "            os.mkdir(dir)\n",
        "\n",
        "    for (i, label) in enumerate(labels):\n",
        "        output_filename = path.join(output_dirs[label], str(i) + \".jpg\")\n",
        "        print(\"Writing \" + output_filename)\n",
        "\n",
        "        with open(output_filename, \"wb\") as h:\n",
        "            data_i = [\n",
        "                data[ (i*rows*cols) + j*rows : (i*rows*cols) + (j+1)*rows ] for j in range(rows)\n",
        "            ]\n",
        "            data_array = np.asarray(data_i)\n",
        "\n",
        "            im = Image.fromarray(data_array)\n",
        "\n",
        "\n",
        "            im.save(output_filename)\n",
        "\n",
        "for dataset in [\"training\", \"testing\"]:\n",
        "    write_dataset(*read(dataset), path.join(\"data2\", dataset))\n",
        "\"\"\"\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, dir_path):\n",
        "        self.dir_path = dir_path\n",
        "\n",
        "        self.len_dataset = 0\n",
        "        self.data_list = []\n",
        "\n",
        "        data_tree = os.walk(dir_path)\n",
        "        for path_dir, dir_list, file_list in data_tree:\n",
        "            if path_dir == dir_path:\n",
        "                self.classes = dir_list\n",
        "                self.class_to_index = {\n",
        "                    class_name: i for i, class_name in enumerate(self.classes)\n",
        "                }\n",
        "                continue\n",
        "\n",
        "            current_class = path_dir.split('/')[-1]\n",
        "\n",
        "            for name in file_list:\n",
        "                file_path = os.path.join(path_dir, name)\n",
        "                self.data_list.append((file_path, self.class_to_index[current_class]))\n",
        "            self.len_dataset += len(file_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len_dataset\n",
        "    def len(self):\n",
        "        return self.len_dataset\n",
        "    def __getitem__(self, index):\n",
        "        file_path, target = self.data_list[index]\n",
        "        image = np.array(Image.open(file_path))\n",
        "        return image, target\n",
        "\n",
        "dataset_images = CustomImageDataset(\"data2/training\")\n",
        "dataloader_images = DataLoader(dataset=dataset_images, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "WaMZ0h3e99w6"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}